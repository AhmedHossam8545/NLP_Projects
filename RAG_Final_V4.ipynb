{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit faiss-cpu google-generativeai langchain langchain-community tiktoken pdfplumber\n",
        "!pip install pyngrok\n"
      ],
      "metadata": {
        "id": "PH7Laeb7Vocq"
      },
      "id": "PH7Laeb7Vocq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6744fd53",
      "metadata": {
        "id": "6744fd53"
      },
      "outputs": [],
      "source": [
        "\n",
        "%%writefile app.py\n",
        "import os\n",
        "import streamlit as st\n",
        "import requests\n",
        "import pickle\n",
        "import pdfplumber\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from typing import List, Tuple\n",
        "\n",
        "EMBED_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "EMBED_DIM = EMBED_MODEL.get_sentence_embedding_dimension()\n",
        "INDEX_FILE = \"faiss_index.bin\"\n",
        "CHUNKS_FILE = \"text_chunks.pkl\"\n",
        "\n",
        "\n",
        "\n",
        "# ==============================================\n",
        "# Text Processing\n",
        "# ==============================================\n",
        "def extract_text_from_pdf(pdf_path: str) -> str:\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "    return text\n",
        "\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = 400, overlap: int = 80) -> List[str]:\n",
        "    \"\"\"Split long text into overlapping chunks.\"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(words):\n",
        "        end = min(start + chunk_size, len(words))\n",
        "        chunks.append(\" \".join(words[start:end]))\n",
        "        start += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ==============================================\n",
        "# Embeddings & FAISS Indexing\n",
        "# ==============================================\n",
        "def build_faiss_index(chunks: List[str]) -> Tuple[faiss.IndexFlatL2, np.ndarray]:\n",
        "    \"\"\"Build FAISS index from text chunks.\"\"\"\n",
        "    embeddings = EMBED_MODEL.encode(chunks, convert_to_numpy=True, show_progress_bar=True)\n",
        "    index = faiss.IndexFlatL2(EMBED_DIM)\n",
        "    index.add(embeddings)\n",
        "    return index, embeddings\n",
        "\n",
        "\n",
        "def save_index(index, chunks):\n",
        "    \"\"\"Save FAISS index and chunks to disk.\"\"\"\n",
        "    faiss.write_index(index, INDEX_FILE)\n",
        "    with open(CHUNKS_FILE, \"wb\") as f:\n",
        "        pickle.dump(chunks, f)\n",
        "\n",
        "\n",
        "def load_index() -> Tuple[faiss.IndexFlatL2, List[str]]:\n",
        "    \"\"\"Load FAISS index and text chunks.\"\"\"\n",
        "    if not os.path.exists(INDEX_FILE) or not os.path.exists(CHUNKS_FILE):\n",
        "        return None, []\n",
        "    index = faiss.read_index(INDEX_FILE)\n",
        "    with open(CHUNKS_FILE, \"rb\") as f:\n",
        "        chunks = pickle.load(f)\n",
        "    return index, chunks\n",
        "\n",
        "\n",
        "\n",
        "# ==============================================\n",
        "# Retrieval from FAISS\n",
        "# ==============================================\n",
        "def retrieve_relevant_chunks(query: str, index, chunks: List[str], top_k: int = 3) -> List[Tuple[str, float]]:\n",
        "    \"\"\"Retrieve top-k most similar chunks from FAISS index.\"\"\"\n",
        "    query_emb = EMBED_MODEL.encode([query], convert_to_numpy=True)\n",
        "    distances, indices = index.search(query_emb, top_k)\n",
        "    results = []\n",
        "    for idx, dist in zip(indices[0], distances[0]):\n",
        "        if 0 <= idx < len(chunks):\n",
        "            results.append((chunks[idx], dist))\n",
        "    return results\n",
        "\n",
        "\n",
        "# ==============================================\n",
        "# Gemini API Call\n",
        "# ==============================================\n",
        "def call_gemini_apii(prompt: str, model: str = \"gemini-2.0-flash\", max_tokens: int = 512, temperature: float = 0.3) -> str:\n",
        "    import os\n",
        "    import google.generativeai as genai\n",
        "\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyA9bbIRYRq-JQYDK6490FY6QCAdk6JInbA\"\n",
        "    genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "    try:\n",
        "        model_instance = genai.GenerativeModel(model)\n",
        "        response = model_instance.generate_content(\n",
        "            prompt,\n",
        "            generation_config={\n",
        "                \"temperature\": temperature,\n",
        "                \"max_output_tokens\": max_tokens\n",
        "            }\n",
        "        )\n",
        "\n",
        "        if hasattr(response, \"text\") and response.text:\n",
        "            return response.text\n",
        "        else:\n",
        "            finish_reason = None\n",
        "            if response.candidates:\n",
        "                finish_reason = response.candidates[0].finish_reason\n",
        "            return f\"‚ö†Ô∏è No valid text output. Finish reason: {finish_reason}\\n\\nFull Response:\\n{response}\"\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error calling Gemini API: {e}\"\n",
        "\n",
        "\n",
        "\n",
        "# ==============================================\n",
        "# RAG Prompt Construction\n",
        "# ==============================================\n",
        "def build_rag_prompt(question: str, retrieved_chunks: List[Tuple[str, float]]) -> str:\n",
        "    \"\"\"Combine retrieved text chunks with the user question.\"\"\"\n",
        "    context = \"\\n\\n\".join([f\"Source {i+1}:\\n{chunk}\" for i, (chunk, _) in enumerate(retrieved_chunks)])\n",
        "    prompt = f\"\"\"\n",
        "You are an AI assistant. Use the following document excerpts to answer the question accurately and concisely.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer with reasoning and cite sources like (Source 1), (Source 2), etc.\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyA9bbIRYRq-JQYDK6490FY6QCAdk6JInbA\"\n",
        "\n",
        "print(\"Gemini API key loaded:\", \"GOOGLE_API_KEY\" in os.environ)\n",
        "\n",
        "\n",
        "\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import TextLoader\n",
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "model = genai.GenerativeModel(\"gemini-2.5-pro\")\n",
        "\n",
        "text = \"Machine learning is a subset of AI that uses data to learn patterns.\"\n",
        "docs = [{\"content\": text, \"source\": \"test.txt\"}]\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "texts = [d[\"content\"] for d in docs]\n",
        "faiss_index = FAISS.from_texts(texts, embeddings)\n",
        "\n",
        "query = \"What is machine learning?\"\n",
        "retrieved = faiss_index.similarity_search(query, k=1)\n",
        "context = retrieved[0].page_content\n",
        "response = model.generate_content(f\"{query}\\nContext: {context}\")\n",
        "print(\"Answer:\", response.text)\n",
        "\n",
        "\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "# genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# List available models\n",
        "for m in genai.list_models():\n",
        "  print(f\"Model: {m.name}, Supported methods: {m.supported_generation_methods}\")\n",
        "\n",
        "\n",
        "# ==============================================\n",
        "# Streamlit\n",
        "# ==============================================\n",
        "import os\n",
        "import streamlit as st\n",
        "import requests\n",
        "import pickle\n",
        "import pdfplumber\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from typing import List, Tuple\n",
        "import streamlit as st\n",
        "import pickle, json\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import pdfplumber\n",
        "\n",
        "st.set_page_config(page_title=\"üìö RAG Q&A with Gemini\", layout=\"wide\")\n",
        "st.title(\"üìö Retrieval-Augmented Generation (RAG) using FAISS + Gemini API\")\n",
        "\n",
        "st.sidebar.header(\"üîß Options\")\n",
        "mode = st.sidebar.radio(\"Choose Action:\", [\"Upload & Index\", \"Ask Questions\"])\n",
        "top_k = st.sidebar.slider(\"Top K Chunks:\", 1, 10, 3)\n",
        "EMBED_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "EMBED_DIM = EMBED_MODEL.get_sentence_embedding_dimension()\n",
        "INDEX_FILE = \"faiss_index.bin\"\n",
        "CHUNKS_FILE = \"text_chunks.pkl\"\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyA9bbIRYRq-JQYDK6490FY6QCAdk6JInbA\"\n",
        "\n",
        "print(\"Gemini API key loaded:\", \"GOOGLE_API_KEY\" in os.environ)\n",
        "\n",
        "if mode == \"Upload & Index\":\n",
        "    uploaded_files = st.file_uploader(\"Upload one or more PDFs\", accept_multiple_files=True, type=[\"pdf\"])\n",
        "    if uploaded_files:\n",
        "        all_text = \"\"\n",
        "        for file in uploaded_files:\n",
        "            pdf_path = f\"./{file.name}\"\n",
        "            with open(pdf_path, \"wb\") as f:\n",
        "                f.write(file.read())\n",
        "            all_text += extract_text_from_pdf(pdf_path) + \"\\n\"\n",
        "\n",
        "        st.info(\"Splitting text into chunks and creating FAISS index...\")\n",
        "        chunks = chunk_text(all_text)\n",
        "        index, _ = build_faiss_index(chunks)\n",
        "        save_index(index, chunks)\n",
        "        st.success(\"‚úÖ Index built and saved successfully!\")\n",
        "\n",
        "elif mode == \"Ask Questions\":\n",
        "    index, chunks = load_index()\n",
        "    if index is None:\n",
        "        st.warning(\"‚ö†Ô∏è Please upload and build an index first.\")\n",
        "    else:\n",
        "        question = st.text_input(\"üí¨ Ask a question about your documents:\")\n",
        "        if st.button(\"Generate Answer\") and question:\n",
        "            st.info(\"üîç Retrieving relevant text chunks...\")\n",
        "            retrieved = retrieve_relevant_chunks(question, index, chunks, top_k)\n",
        "            st.write(\"**Retrieved Sources:**\")\n",
        "            for i, (chunk, dist) in enumerate(retrieved):\n",
        "                with st.expander(f\"Source {i+1} (distance={dist:.2f})\"):\n",
        "                    st.write(chunk)\n",
        "\n",
        "            prompt = build_rag_prompt(question, retrieved)\n",
        "            st.info(\"ü§ñ Generating answer using Gemini...\")\n",
        "            answer = call_gemini_apii(prompt)\n",
        "            st.subheader(\"üß† Generated Answer:\")\n",
        "            st.write(answer)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },

  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
